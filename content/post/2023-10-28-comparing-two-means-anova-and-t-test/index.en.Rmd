---
title: Comparing Two means (ANOVA and T.test)
author: Bongani Ncube
date: '2023-10-28'
slug: comparing-means-anova-and-t-test
categories:
  - ANOVA
  - T.TEST
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2023-10-28T18:36:56+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
<script src="//yihui.org/js/math-code.js" defer></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script defer
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, 
                      echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE,
                      fig.path = "static",
                      fig.height=6, 
                      fig.width = 1.777777*6,
                      fig.align='center',
                      tidy = FALSE, 
                      comment = NA, 
                      highlight = TRUE, 
                      prompt = FALSE, 
                      crop = TRUE,
                      comment = "#>",
                      collapse = TRUE)
knitr::opts_knit$set(width = 60)
library(tidyverse)
library(reshape2)
theme_set(theme_light(base_size = 16))
make_latex_decorator <- function(output, otherwise) {
  function() {
      if (knitr:::is_latex_output()) output else otherwise
  }
}
insert_pause <- make_latex_decorator(". . .", "\n")
insert_slide_break <- make_latex_decorator("----", "\n")
insert_inc_bullet <- make_latex_decorator("> *", "*")
insert_html_math <- make_latex_decorator("", "$$")
```

## Motivation

As a statistics tutor ,One of the question i always encounter is _How do i apply statistics in real life_ , like **Zvinoshanda here zvinhu izvi** , well the answer is yes. With the rise in popularity of big data , statistical literacy is as important as other literacies . Knowing how to handle and draw conclusions is a must

### T-test

When you have a single explanatory variable which is qualitative and only have **two levels**, you
can run a *student's T-test* to test for a difference in the mean of the
two levels. If appropriate for your data, you can choose to test a
unilateral hypothesis. This means that you can test the more specific
assumption that one level has a higher mean than the other, rather than
that they simply have different means.Note that robustness of this test increases with sample size and is higher when groups have equal sizes


For the t-test, the t statistic used to find the p-value calculation is calculated as:
$t = (\overline{y_{1}}-\overline{y_{2}})/\sqrt{\frac{s_{1}^2} n_{1} + \frac{s_{2}^2} n_{2}}$

where

$\overline{y_{1}}$ and $\overline{y_{2}}$ are the means of the response variable y for group 1 and 2, respectively,\
$s_{1}^2$ and $s_{2}^2$ are the variances of the response variable y for group 1 and 2, respectively,\
$n_{1}$ and $n_{2}$ are the sample sizes of groups 1 and 2, respectively.


Note that the t-test is mathematically equivalent to a one-way ANOVA
with 2 levels.

# Assumptions

If the assumptions of the t-test are not met, the test can give
misleading results. Here are some important things to note when testing
the assumptions of a t-test.

1.  **Normality of data**\
    As with simple linear regression, the residuals need to be normally
    distributed. If the data are not normally distributed, but have
    reasonably symmetrical distributions, a mean which is close to the
    centre of the distribution, and only one mode (highest point in the
    frequency histogram) then a t-test will still work as long as the
    sample is sufficiently large (rule of thumb \~30 observations). If
    the data is heavily skewed, then we may need a very large sample
    before a t-test works. In such cases, an alternate non-parametric
    test should be used.
    
2.  **Homoscedasticity**\
    Another important assumption of the two-sample t-test is that the
    variance of your two samples are equal. This allows you to calculate
    a pooled variance, which in turn is used to calculate the standard
    error. If population variances are unequal, then the probability of
    a Type I error is greater than $\alpha$.\
    The robustness of the t-test increases with sample size and is
    higher when groups have equal sizes.\
    We can test for difference in variances among two populations and
    ask what is the probability of taking two samples from two
    populations having identical variances and have the two sample
    variances be as different as are $s_{1}^2$ and $s_{2}^2$.\
    To do so, we must do the variance ratio test (i.e. an F-test).
    
    
# Running a t-test (Question from Zou Module)

 A psychologist gave a test to 10 females and 10 males to test the hypothesis that 
males should score higher on the tests. The following scores were obtained.

```{r}
Females<- c(20,	15,	19,	18,13,	14,	16,	12,	15,	25)
Males<-c(20	,16	,24	,11	,23,	25,	22,	24,	26,	28)

df<-cbind(Females,Males) %>% 
   data.frame() 

df %>% rownames_to_column(var = " ") %>%
  janitor::adorn_totals(where = c("row", "col")) %>%
  flextable::flextable() %>%
  flextable::colformat_int(j = c(2, 3, 4), big.mark = ",") %>%
  flextable::autofit()

```
Assuming that the populations are normal with equal variance test the claim at 5% level of 
 Significance run a t test. (20). 

## In R 

+ R understands both the t.test and anova more oftenly in long format rather than wide format

```{r}

df_new<-df |> 
  gather("gender","value")
ggplot(df_new,aes(x=gender,y=value,fill=gender))+geom_boxplot()
```

> There seem to be a difference just by looking at the boxplot. Let's make sure are assumptions are met before doing the test.

```{r, echo = TRUE, eval = TRUE}
# Assumption of equal variance
vattest<-var.test(value~gender,data=df_new)
vattest
```
**comment**

```{r}
vattest$p.value
```

> the _p-value_ is greater than **0.05** indicating theat that there is no significance difference in the variances for males and females.

```{r}
vattest$estimate
```
> the ratio is also close to one hence the assumption of homogeniety holds

## Running the test mannually

$$\bar{X}_{males}=21.9$$ $$\bar{X}_{females}=16.7$$

## Hypothesis

+ H0: $\bar{X}_{males}=\bar{X}_{females}$ 
+ H1: $\bar{X}_{males}>\bar{X}_{females}$

$$S^2_{females}=\frac{\sum(x-\bar{x}_{females})^2}{n-1}$$
$$S^2_{females}=\frac{(20-16.7)^2+(15-16.7)^2+...+(25-16.7)^2}{9}=15.12222$$

$$S^2_{males}=\frac{\sum(x-\bar{x}_{males})^2}{n-1}$$
$$S^2_{males}=\frac{(20-21.9)^2+(16-21.9)^2+...+(28-21.9)^2}{9}=25.65556$$

$$S_{pooled}=\frac{(N_{males}-1)S^2_{males}+(N_{females}-1)S^2_{females}}{N_{males}+N_{females}-2}$$

$$S_{pooled}=\frac{(9)(25.65556)+(9)(15.12222)}{18}=20.38889$$

## test statistic

$$t=\frac{\bar{X}_{males}-\bar{X}_{females}}{S_{pooled}(\frac{1}{N_{males}}+\frac{1}{N_{females}})}$$

$$t=\frac{21.9-16.7}{20.38889(2/10)}=2.575085$$

$$t_{0.05}(18)=1.734$$

## Conclusion

since $t>t_{0.05}(18)$ we reject the null hypothesis and conclude that
there is evidence to prove that the means are not equal and suggest that
the average for males is greater than that of females.

## In R, t-tests are implemented using the function `t.test`. 

```{r, echo = TRUE, eval = FALSE}
t.test(Y ~ X2, data= data, alternative = c("greater","less","two.sided"),var.equal=TRUE)
```

```{r}
# or equivalently
ttest1 <- t.test(value~gender, var.equal=TRUE,alternative="greater",data=df_new)
ttest1

```
_var.equal=TRUE_ implies we are enforcing the restriction that the variances are equal

```{r}
-1*ttest1$statistic
```
> this is the value we calculated earlier

```{r}
ttest1$p.value
```
> the p-value is greater than 0.05 hence we conclude that the true difference in means between group Females and group Males is greater than 0



## Analysis of Variance (ANOVA)

Analysis of Variance (ANOVA) is a type of `linear model` for a continuous
response variable and one or more categorical explanatory variables. The
categorical explanatory variables can have any number of levels
(groups). For example, the variable race might have three levels:
Black,White and Hispanic. ANOVA tests whether the means of the response
variable differ between the levels by comparing the variation within a 
group with the variation among groups. For example, if mean `IQ` for those who are black differs with those who are white.


{{% callout note %}}
ANOVA calculations are based on the `sum of squares partitioning` and
compares the :

- [x] `within-level variance` to the 
- [x] `between-level variance`. 

>If the between-level variance is greater than the
within-level variance, this means that the levels affect the
explanatory variable more than the random error (corresponding to the
within-level variance), and that the explanatory variable is likely
to be significantly influenced by the levels.

{{% /callout %}}


```{r, echo = FALSE, fig.height=3, fig.width=6.5}
source('images/figAnova.R')
```

## Doing the calculation 

In the ANOVA, the comparison of the between-level variance to the
within-level variance is made through the calculation of the
`F-statistic` that correspond to the ratio of the mean sum of squares of
the level (MS~Lev~) on the mean sum of squares of the error (MS~$\epsilon$~).
These two last terms are obtained by dividing their two respective sums
of squares by their corresponding degrees of freedom, as is typically
presented in a ANOVA table . Finally, the p-value of the ANOVA is calculated 
from the F-statistic that follows a Chi-square (χ^2^) distribution.

## Anova Table

 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  Source of\   Degrees of\         Sums of squares                                                  Mean squares                                                 F-statistic
  variation    freedom (df)
                                                           
  ------------ ------------------- -------------------------------------------------------------- ------------------------------------------------------ -------------------------------------- -----------------------------------
  Total        $ra-1$               $SS_{t}=\sum x^2 - \frac{G^2}{N}$

  Facteur A     $a-1$               $SS_{f}=\frac{\sum{T_j^2}}{n}-\frac{G^2}{N}$                   $MS_{f}=\frac{SS_{f}}{(a-1)}$                            $F=\frac{MS_{f}} {MS_{E}}$

  Error        $a(r-1)$               $SS_{\epsilon}=SS_{total}  – SS_{group}$                           $MS_{\epsilon}=\frac{SS_{\epsilon}}{a(r-1)}$
  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

let $T_j$ be total for the $j_{th}$ group and
Grand total=G

## QUESTION 

To investigate maternal behavior of laboratory rats, we separated the rat pup from the 
mother and recorded the time (in seconds) required for the mother to retrieve the pup. 
We ran the study with 5-, 20-, and 35 – day – old pups because we were interested in 
whether retrieval time various with the age of pup. The data are given below, where 
there are six pups per group.



| day 5 | day 20 | day 35 |
|-------|--------|--------|
| 10    | 15     | 35     |
| 15    | 30     | 40     |
| 25    | 20     | 50     |
| 15    | 15     | 43     |
| 20    | 23     | 45     |
| 18    | 20     | 40     |
| 103   | 123    | 253    |

### Calculating the test statistic

let $T_j$ be total for the $j_{th}$ group $T_5=103$ ,$T_{20}=123$ and
$T_{35}=253$ Grand total=G=103+123+253=479

$$SS_{total}=\sum x^2 - \frac{G^2}{N}$$
$$SS_{total} =(10^2+15^2+25^2+ ... + 45^2+40^2)-479^2/18=15377-12746.72$$

$$SS_{total}=2630.278$$

$$SS_{group}=\frac{\sum{T_j^2}}{n}-\frac{G^2}{N}$$

$$SS_{group}=\frac{103^2+123^2+253^2}{6}-\frac{479^2}{18}$$

$$SS_{group}=\frac{89747}{6}-\frac{479^2}{18}$$ $$SS_{group}=2211.111$$

$$SS_{error}  = SS_{total}  – SS_{group}$$\
$$SS_{error}  = 419.167$$

| Source | df  | SS       | MS       | F        |
|--------|-----|----------|----------|----------|
| Groups | 2   | 2211.111 | 1105.555 | _39.56257_ |
| Error  | 15  | 419.167  | 27.94447 |          |
| Total  | 17  | 2630.278 | 154.7222 |          |


### Define the hypothesis

+ H0 : all Means are equal 
+ H1 : At least one mean is different

### critical value

$$F_{2,15}(0.05)=3.68$$

### **Conclusion** 

Because our obtained F = 39.56257 exceeds
$F_{.05} = 3.68$, we will reject $H_0$ and conclude that the groups were
sampled from populations with different means.


## Doing this in R

```{r}
library(tidyverse)
df<-tribble(~DAY_5 ,~DAY_20,~DAY_35,
 	10,	15,	35,
 	15,	30,	40,
 	25,	20,	50,
 	15,	15,	43,
 	20,	23,	45,
 	18,	20,	40)
df
```
### Look at the means

```{r}
sapply(df,mean)
```
### look at the standard deviation

```{r}
sapply(df,sd)
```
## Change to long format

```{r}
out_new<-df |> 
  gather("day","time")
```


## visually inspect

```{r}
ggplot(out_new,aes(x=day,y=time,fill=day))+geom_boxplot()
```

Let's now run the ANOVA. In R, ANOVA can be called either directly with the `aov` function, or with the `anova` function performed on a linear model implemented with `lm`:

```{r, echo = TRUE}
# Using aov()
aov1 <- aov(time ~ day, data=out_new)
summary(aov1)

# Using lm()
anov1 <- lm(time ~ day, data=out_new)
anova(anov1)
```

## breaking down the output (Conclusion from R)

+ _F-value=39.56_ just as we calculated before
+ _Pr(F)_ is less than 5 percent indicating that the means are significantly different

### Complementary test

Importantly, ANOVA cannot identify which treatment is different from the
others in terms of response variable. It can only identify that a
difference is present. To determine the location of the difference(s),
post-hoc tests that compare the levels of the explanatory variables
(i.e. the treatments) two by two, must be performed. While several
post-hoc tests exist (e.g. Fischer's least significant difference,
Duncan's new multiple range test, Newman-Keuls method, Dunnett's test,
etc.), the Tukey's range test is used in this example using the function
`TukeyHSD` as follows:

```{r, echo = TRUE, eval = FALSE}
# Where does the Diet difference lie?
TukeyHSD(aov(anov1),ordered=T)

# or equivalently
TukeyHSD(aov1,ordered=T)
```
## Conclusion

+ looking at the column with _p adj_ , values that are less than `0.05` indicate that the pair has a significant difference in means thus pairs `DAY_35-DAY_5` and `DAY_35-DAY_20` are are significant

## the plot

```{r}
tkplot<-TukeyHSD(aov1,ordered=T)
plot(tkplot)
```

