---
title: "Gentle introduction to Logistic Regression"
author: "Bongani Ncube"
date: "2023-10-02"
output:
  
  rmdformats::downcute: null
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
editor_options:
  markdown:
    wrap: 72
---

```{r , include = FALSE}
knitr::opts_chunk$set(cache = TRUE, 
                      echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE,
                      fig.path = "static",
                      fig.height=6, 
                      fig.width = 1.777777*6,
                      fig.align='center',
                      tidy = FALSE, 
                      comment = NA, 
                      highlight = TRUE, 
                      prompt = FALSE, 
                      crop = TRUE,
                      comment = "#>",
                      collapse = TRUE)
knitr::opts_knit$set(width = 60)
library(tidyverse)
library(reshape2)
theme_set(theme_light(base_size = 16))
make_latex_decorator <- function(output, otherwise) {
  function() {
      if (knitr:::is_latex_output()) output else otherwise
  }
}
insert_pause <- make_latex_decorator(". . .", "\n")
insert_slide_break <- make_latex_decorator("----", "\n")
insert_inc_bullet <- make_latex_decorator("> *", "*")
insert_html_math <- make_latex_decorator("", "$$")
```

```{r,echo=FALSE}
knitr::include_graphics("associate.png")
```

### Introduction

This article looks at how to interpret the output of the `glm()` R function using the *ATTRITION DATASET* train dataset.

A note on the **p-value**: the p-value is a test of significance for the null hypothesis $H_0$ that

* there is no difference in the log-odds of the outcome between the reference group (captured by the intercept) and the explanatory variable (or one of its categories), or that the difference between the two groups equals zero: $H_0: b_1 = 0$ and $H_a: b_1 \neq 0$.

If p<0.05, we reject $H_0$ as we have evidence to suggest that the difference between the two groups does not equal zero.

## Types Of Modeling Techniques

```{r, echo = FALSE}
library(tidyverse)
library(DT)

d <- data.frame(
  Outcome = c(
    "Continuous scale (eg money, height, weight)",
    "Binary scale (Yes/No)",
    "Nominal category scale (eg A, B, C)",
    "Ordinal category scale (eg Low, Medium, High)",
    "Time dependent binary scale"
  ),
  Model = c(
    "Linear regression",
    "Binomial logistic regression",
    "Multinomial logistic regression",
    "Ordinal logistic regression",
    "attrition/proportional hazard regression"
  )
)

datatable(d,style="bootstrap", class="table-condensed",
          options = list(dom = 'tp', scrollX = TRUE))
```


## Logistic regression

When you have a binary outcome (Yes/No), you can use a chi square test to compare the differences in proportions across $n$ number of groups. For instance, if you had two groups (exposed and unexposed) and a binary outcome (event and no event), you can create a 2 x 2 contingency table and use a chi square test to test if there is a difference in the frequency or proportion in the outcome across the two groups. However, this will not get you the magnitude of the differences, the direction of the difference, nor the uncertainty with the differences. 


## Why not linear regression
### Generate an example dataset

```{r}
set.seed(12345)

interest=rnorm(20,175,20) 

interest=sort(interest) 

loan_status=c(0,0,0,0,0,1,0,1,0,0,1,1,0,1,1,1,0,1,1,1) 

loan_data=as.data.frame(cbind(interest,loan_status)) 
```

## Linear Regression Model

```{r}
lm1 <- lm(loan_status ~ interest, data = loan_data)
summary(lm1)
```

## Takeaways

+ interest rate has a significant positive effect on `loan status` (a bit silly)
+ the model is overally significant and better than the null model because `p-value<0.05`
+ the `R squared` and `adj R sqred` are significantly small


### logistic regression model

```{r}
blr1 <- glm(loan_status ~ interest, family = binomial, data = loan_data)
summary(blr1)
```

## takeaways 

+ interest rate still has a positive effect on `loan status`

### Compare the results graphically

```{r}
loan_data <- loan_data %>%
  dplyr::mutate(Pred_lm = predict(lm1, loan_data),
                Pred_blm = predict(blr1, loan_data)) %>%
  dplyr::mutate(Prob_lm = plogis(predict(lm1, loan_data)),
                Prob_blm = plogis(predict(blr1, loan_data)),
                Pred_blm2 = ifelse(predict(blr1, loan_data, type = "response") >= .5, 1, 0))
 
p1 <- ggplot(loan_data, aes(x = interest, y =  loan_status)) +
  geom_point() +
  geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], color = "red", size = .5) +
  geom_point(aes(x = interest, y = Pred_lm), color = "blue") +
  tvthemes::theme_avatar()+
  coord_cartesian(ylim = c(-0.2, 1.2), xlim = c(125, 225)) +
  scale_y_continuous(breaks=seq(0, 1, 1), labels = c("non-default", "default")) +
  guides(fill = FALSE) +
  labs(title = "linear regression", x = "amount", y = "")


p2 <- ggplot(loan_data, aes(x = interest, y =  loan_status)) +
  geom_point() +
  geom_point(aes(x = interest, y = Prob_blm), color = "blue") +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              se = FALSE, color = "red", size = .5) +
  geom_segment(aes(xend = interest, yend = Prob_blm), color = "red", alpha = .2) +
 tvthemes::theme_avatar()+
  coord_cartesian(ylim = c(-0.2, 1.2), xlim = c(125, 225)) +
  scale_y_continuous(breaks=seq(0, 1, 1), labels = c("non-default", "default")) +
  guides(fill = FALSE) +
  labs(title = "logistic regression", x = "amount", y = "",
       caption="Bongani Ncube")

ggpubr::ggarrange(p1, p2, ncol =2, nrow = 1)
```

## Deriving Logistic Regression

The structural form of the logistic regression model:

<div align="center">$logit( E[Y_i | X_i]) = logit(p_i) = ln(\frac{p_i}{1 - p_i}) = \beta_0 + \beta_1 X_{1i} + \epsilon$</div> \
$$p_i = f(\mathbf{x}_i ; \beta) = \frac{exp(\mathbf{x_i'\beta})}{1 + exp(\mathbf{x_i'\beta})}$$

Equivalently,

$$
logit(p_i) = log(\frac{p_i}{1+p_i}) = \mathbf{x_i'\beta}
$$

where $\frac{p_i}{1+p_i}$is the **odds**.

In this form, the model is specified such that **a function of the mean response is linear**. Hence, **Generalized Linear Models**

The likelihood function

$$L(p_i) = \prod_{i=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}$$



where $p_i = \frac{\mathbf{x'_i \beta}}{1+\mathbf{x'_i \beta}}$ and $1-p_i = (1+ exp(\mathbf{x'_i \beta}))^{-1}$

Hence, our objective function is

$$
Q(\beta) = log(L(\beta)) = \sum_{i=1}^n Y_i \mathbf{x'_i \beta} - \sum_{i=1}^n  log(1+ exp(\mathbf{x'_i \beta}))
$$

## Multivariable logistic regression model {.tabset}

The structural form of the multivariable logistic regression model (this example uses two `X` variables):

<div align="center">$logit( E[Y_i | X_i]) = logit(p_i) = ln(\frac{p_i}{1 - p_i}) = \beta_0 + \beta_1 X_{1i} + + \beta_2 X_{2i} + \epsilon$</div> \

## Interpreting the coefficients

Our coefficients indicate the linear impact on the log odds of a positive decision.  A negative coefficient decreases the log odds and a positive coefficient increases the log odds.   

We can easily extend the manipulations from a few slides back to get a formula for the odds of an event in terms of the coefficents $\beta_0, \beta_1, ..., \beta_p$:

$$
\begin{align*}
\frac{P(y = 1)}{P(y = 0)} &= e^{\beta_0 + \beta_1x_1 + ... + \beta_px_p} \\
&= e^{\beta_0}(e^{\beta_1})^{x_1}...(e^{\beta_p})^{x_p}
\end{align*}
$$

* $e^{\beta_0}$ is the odds of the event assuming zero from all input variables 
* $e^{\beta_i}$ is the multiplier of the odds associated with a one unit increase in $x_i$ (for example, an extra point rating in physical attractiveness), assuming all else equal - because of the multiplicative effect, we call this the *odds ratio* for $x_i$.

### Categorical Explanatory Variable (2 Categories)

* *attrition_status* is modelled as a function of *Sex*.
* Reference group is *female*^[By default R assigns the reference group based on alphabetical order].


```{r}
attrition_data<-readr::read_csv("data-attrition.csv") |> 
  mutate(attrition_status= ifelse(attrition=="yes",1,0)) |> 
  mutate(department=as.factor(department))

model1 <- glm(attrition_status ~ gender, family=binomial(link='logit'), data=attrition_data)
summary(model1)
```

* **Intercept**: The log-odds of attrition for women is `r model1$coefficients[1]`.
* **Sexmale**: 
   * The difference in the log-odds of attrition between men and women is `r model1$coefficients[2]` i.e. the chance of attrition is lower for men than for women.
   * Given $p < 0.05$, we can **reject the null hypothesis** ($b_1=0$) that there is no difference in the log-odds between men and women.
   * Men have `r exp(model1$coefficients[2])`^[log-odds to odds-ratio: exp(log-odds)] times the odds (i.e. `r round((exp(model1$coefficients[2]) - 1) *100,0)` %)^[odds-ratio to %: (OR-1) * 100] of attrition than women.
* *Null deviance* = SSTot
* *Residual deviance* = SSResid

<br>

### Categorical Explanatory Variable (More than 2 Categories)

* *attrition_status* is modeled as a function of *department*.

* The default reference group is *human resources*.

```{r}
model2 <- glm(attrition_status ~ department, family=binomial(link='logit'), 
              data=na.omit(attrition_data))
summary(model2)
```

* **Intercept**: the log-odds of attrition for someone in the `human resources` (reference group) is `r model2$coefficients[1]`.
* For each of the other age groups, the coefficient tells us that the log-odds of attrition for a given group is smaller or bigger than that of the reference group.
* Where $p>0.05$, the **null hypothesis $b_k=0$ cannot be rejected** i.e. there is insufficient statistical evidence that the chance of attrition is significantly smaller compared to the reference group.

<br>

#### Now let's change the reference group to sales:

* this is still based on data **without** missing department rows.

```{r}

levels(attrition_data$department)
attrition_data$departmentR <- relevel(attrition_data$department, ref=2)
levels(attrition_data$departmentR) # new col is not attached so must use data$
model3 <- glm(attrition_status ~ departmentR, family=binomial(link='logit'), 
              data=na.omit(attrition_data))
summary(model3)
```

.

<br>



### Continuous Explanatory Variable

* *attrition_status* is modelled as a function of *Age*.
* There is no reference group as such.
* **R "can deal with" missing data for a continuous variable** i.e.throws no error due to missing data in the continuous variable.

```{r}
model4 <- glm(attrition_status ~ age, family=binomial(link='logit'), data=attrition_data)
summary(model4)
```

* **Intercept**: The log-odds of attrition when $Age = 0$ is `r model4$coefficients[1]`.
* **Age**: 
   * For every unit increase in *Age* the log-odds of attrition decrease by `r model4$coefficients[2]` i.e. the chances of attrition decrease as employee`s age increases.
   * Given $p < 0.05$, we can **reject the null hypothesis** $b_1=0$ that one unit increase in age does not affect chances of attrition.
   * For every unit increase in *Age*, the odds of attrition are `r exp(model4$coefficients[2])` times the odds of those with one *Age* unit less (i.e. `r round((exp(model4$coefficients[2]) - 1) *100, 3)` %).
   




